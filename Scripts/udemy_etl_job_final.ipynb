{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %timeout            Int           The number of minutes after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session.\n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n    %reconnect          String        Specify a live session ID to switch/reconnect to the sessions.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming and etl.\n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n    %matplot      Matplotlib figure   Visualize your data using the matplotlib library.\n                                      E.g. \n                                      import matplotlib.pyplot as plt\n                                      # Set X-axis and Y-axis values\n                                      x = [5, 2, 8, 4, 9]\n                                      y = [10, 4, 8, 5, 2]\n                                      # Create a bar chart \n                                      plt.bar(x, y) \n                                      # Show the plot\n                                      %matplot plt    \n    %plotly            Plotly figure  Visualize your data using the plotly library.\n                                      E.g.\n                                      import plotly.express as px\n                                      #Create a graphical figure\n                                      fig = px.line(x=[\"a\",\"b\",\"c\"], y=[1,3,2], title=\"sample figure\")\n                                      #Show the figure\n                                      %plotly fig\n\n  \n                \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql.functions import col, when, count, lit, trim, avg, sum, countDistinct\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 6c80da70-1b8e-4e5f-9820-e5684486f8e9\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session 6c80da70-1b8e-4e5f-9820-e5684486f8e9 to get into ready status...\nSession 6c80da70-1b8e-4e5f-9820-e5684486f8e9 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "\ncourses_dynamic_df = glueContext.create_dynamic_frame.from_catalog(\n    database=\"udemy_database\", \n    table_name=\"courses\"\n)\n\n# Load comments data\ncomments_dynamic_df = glueContext.create_dynamic_frame.from_catalog(\n    database=\"udemy_database\", \n    table_name=\"comments\"\n)\n\n# Convert DynamicFrames to DataFrames for easier transformations\ncourses_df = courses_dynamic_df.toDF()\ncomments_df = comments_dynamic_df.toDF()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Show first 5 rows to verify data is loaded\nprint(\"Courses Data:\")\ncourses_df.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "Courses Data:\n+------+--------------------+-------+------+--------------------+---------------+----------+-----------+------------+------------+------------------+--------------------+----------------+---------+-------------------+-------------+--------+--------------------+---------------+------------------+\n|    id|               title|is_paid| price|            headline|num_subscribers|avg_rating|num_reviews|num_comments|num_lectures|content_length_min|      published_time|last_update_date| category|        subcategory|        topic|language|          course_url|instructor_name|    instructor_url|\n+------+--------------------+-------+------+--------------------+---------------+----------+-----------+------------+------------+------------------+--------------------+----------------+---------+-------------------+-------------+--------+--------------------+---------------+------------------+\n|4715.0|Online Vegan Vege...|   true| 24.99|Learn to cook del...|         2231.0|      3.75|      134.0|        42.0|        37.0|            1268.0|2010-08-05T22:06:13Z|      2020-11-06|Lifestyle|    Food & Beverage|Vegan Cooking| English|/course/vegan-veg...|    Angela Poch| /user/angelapoch/|\n|1769.0|The Lean Startup ...|  false|   0.0|Debunking Myths o...|        26474.0|       4.5|      709.0|       112.0|         9.0|              88.0|2010-01-12T18:09:46Z|                | Business|   Entrepreneurship| Lean Startup| English|/course/the-lean-...|      Eric Ries|   /user/ericries/|\n|5664.0|How To Become a V...|   true| 19.99|Get the tools you...|         1713.0|       4.4|       41.0|        13.0|        14.0|              82.0|2010-10-13T18:07:17Z|      2019-10-09|Lifestyle|    Other Lifestyle|Vegan Cooking| English|/course/see-my-pe...|    Angela Poch| /user/angelapoch/|\n|7723.0|How to Train a Puppy|   true|199.99|Train your puppy ...|         4988.0|       4.8|      395.0|        88.0|        36.0|            1511.0|2011-06-20T20:08:38Z|      2016-01-13|Lifestyle|Pet Care & Training| Pet Training| English|/course/complete-...|     Ian Dunbar| /user/ian-dunbar/|\n|8157.0|Web Design from t...|   true|159.99|Learn web design ...|         1266.0|      4.75|       38.0|        12.0|        38.0|             569.0|2011-06-23T18:31:20Z|                |   Design|         Web Design|   Web Design| English|/course/web-desig...| E Learning Lab|/user/edwin-ang-2/|\n+------+--------------------+-------+------+--------------------+---------------+----------+-----------+------------+------------+------------------+--------------------+----------------+---------+-------------------+-------------+--------+--------------------+---------------+------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"Comments Data:\")\ncomments_df.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Comments Data:\n+---------+---------+----+--------------------+-------------+--------------------+\n|       id|course_id|rate|                date| display_name|             comment|\n+---------+---------+----+--------------------+-------------+--------------------+\n| 88962892|  3173036| 1.0|2021-06-29T18:54:...|        Rahul|I think a beginne...|\n|125535470|  4913148| 5.0|2022-10-07T11:17:...|        Marlo|Aviva is such a n...|\n| 68767147|  3178386| 3.5|2020-10-19T06:35:...|Yamila Andrea|Muy buena la intr...|\n|125029758|  3175814| 5.0|2022-09-30T21:13:...|   Jacqueline|This course is th...|\n| 76584052|  3174896| 4.5|2021-01-30T08:45:...|      Anthony|I found this cour...|\n+---------+---------+----+--------------------+-------------+--------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Replace empty strings and other non-standard null representations with actual nulls\nfrom pyspark.sql.functions import when\n\ncourses_df = courses_df.withColumn(\n    \"headline\", when(col(\"headline\") == \"\", None).otherwise(col(\"headline\"))\n).withColumn(\n    \"last_update_date\", when(col(\"last_update_date\") == \"\", None).otherwise(col(\"last_update_date\"))\n)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Convert default placeholders to null\ncourses_df = courses_df.withColumn(\n    \"price\", when(col(\"price\") == 0.0, None).otherwise(col(\"price\"))\n).withColumn(\n    \"headline\", when(col(\"headline\") == \"\", None).otherwise(col(\"headline\"))\n)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Inspect schema of the DataFrame\ncourses_df.printSchema()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- id: double (nullable = true)\n |-- title: string (nullable = true)\n |-- is_paid: boolean (nullable = true)\n |-- price: double (nullable = true)\n |-- headline: string (nullable = true)\n |-- num_subscribers: double (nullable = true)\n |-- avg_rating: double (nullable = true)\n |-- num_reviews: double (nullable = true)\n |-- num_comments: double (nullable = true)\n |-- num_lectures: double (nullable = true)\n |-- content_length_min: double (nullable = true)\n |-- published_time: string (nullable = true)\n |-- last_update_date: string (nullable = true)\n |-- category: string (nullable = true)\n |-- subcategory: string (nullable = true)\n |-- topic: string (nullable = true)\n |-- language: string (nullable = true)\n |-- course_url: string (nullable = true)\n |-- instructor_name: string (nullable = true)\n |-- instructor_url: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Check nulls again after cleaning\ncourses_df.select([count(when(col(c).isNull(), c)).alias(c) for c in courses_df.columns]).show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-----+-------+-----+--------+---------------+----------+-----------+------------+------------+------------------+--------------+----------------+--------+-----------+-----+--------+----------+---------------+--------------+\n| id|title|is_paid|price|headline|num_subscribers|avg_rating|num_reviews|num_comments|num_lectures|content_length_min|published_time|last_update_date|category|subcategory|topic|language|course_url|instructor_name|instructor_url|\n+---+-----+-------+-----+--------+---------------+----------+-----------+------------+------------+------------------+--------------+----------------+--------+-----------+-----+--------+----------+---------------+--------------+\n|  0|    0|      0|21738|      27|              0|         0|          0|           0|           0|                 0|             0|             137|       0|          0|    0|       0|         0|              0|             0|\n+---+-----+-------+-----+--------+---------------+----------+-----------+------------+------------+------------------+--------------+----------------+--------+-----------+-----+--------+----------+---------------+--------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Count null-like values\nnull_like_counts = courses_df.select(\n    count(when((col(\"instructor_name\").isNull()) | (col(\"instructor_name\") == \"\") | (col(\"instructor_name\").rlike(\"^\\s*$\")), \"instructor_name\")).alias(\"instructor_name_nulls\"),\n    count(when((col(\"instructor_url\").isNull()) | (col(\"instructor_url\") == \"\") | (col(\"instructor_url\").rlike(\"^\\s*$\")), \"instructor_url\")).alias(\"instructor_url_nulls\"),\n    count(when((col(\"topic\").isNull()) | (col(\"topic\") == \"\") | (col(\"topic\").rlike(\"^\\s*$\")), \"topic\")).alias(\"topic_nulls\")\n)\n\n# Show the counts\nnull_like_counts.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------------------+--------------------+-----------+\n|instructor_name_nulls|instructor_url_nulls|topic_nulls|\n+---------------------+--------------------+-----------+\n|                    5|                 427|        958|\n+---------------------+--------------------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Drop rows where any of the specified columns have null or null-like values\ncourses_df = courses_df.filter(\n    (col(\"instructor_name\").isNotNull()) & (col(\"instructor_name\") != \"\") & (~col(\"instructor_name\").rlike(\"^\\s*$\"))\n).filter(\n    (col(\"instructor_url\").isNotNull()) & (col(\"instructor_url\") != \"\") & (~col(\"instructor_url\").rlike(\"^\\s*$\"))\n).filter(\n    (col(\"topic\").isNotNull()) & (col(\"topic\") != \"\") & (~col(\"topic\").rlike(\"^\\s*$\"))\n).filter(\n    (col(\"headline\").isNotNull()) & (col(\"headline\") != \"\") & (~col(\"headline\").rlike(\"^\\s*$\"))\n)\n\n# Verify the dataset after removal\ncourses_df.show(5)\n\n# Recheck for nulls in the specified columns\ncourses_df.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"instructor_name\", \"instructor_url\", \"topic\", \"headline\"]]).show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+--------------------+-------+------+--------------------+---------------+----------+-----------+------------+------------+------------------+--------------------+----------------+---------+-------------------+-------------+--------+--------------------+---------------+------------------+\n|    id|               title|is_paid| price|            headline|num_subscribers|avg_rating|num_reviews|num_comments|num_lectures|content_length_min|      published_time|last_update_date| category|        subcategory|        topic|language|          course_url|instructor_name|    instructor_url|\n+------+--------------------+-------+------+--------------------+---------------+----------+-----------+------------+------------+------------------+--------------------+----------------+---------+-------------------+-------------+--------+--------------------+---------------+------------------+\n|4715.0|Online Vegan Vege...|   true| 24.99|Learn to cook del...|         2231.0|      3.75|      134.0|        42.0|        37.0|            1268.0|2010-08-05T22:06:13Z|      2020-11-06|Lifestyle|    Food & Beverage|Vegan Cooking| English|/course/vegan-veg...|    Angela Poch| /user/angelapoch/|\n|1769.0|The Lean Startup ...|  false|  null|Debunking Myths o...|        26474.0|       4.5|      709.0|       112.0|         9.0|              88.0|2010-01-12T18:09:46Z|            null| Business|   Entrepreneurship| Lean Startup| English|/course/the-lean-...|      Eric Ries|   /user/ericries/|\n|5664.0|How To Become a V...|   true| 19.99|Get the tools you...|         1713.0|       4.4|       41.0|        13.0|        14.0|              82.0|2010-10-13T18:07:17Z|      2019-10-09|Lifestyle|    Other Lifestyle|Vegan Cooking| English|/course/see-my-pe...|    Angela Poch| /user/angelapoch/|\n|7723.0|How to Train a Puppy|   true|199.99|Train your puppy ...|         4988.0|       4.8|      395.0|        88.0|        36.0|            1511.0|2011-06-20T20:08:38Z|      2016-01-13|Lifestyle|Pet Care & Training| Pet Training| English|/course/complete-...|     Ian Dunbar| /user/ian-dunbar/|\n|8157.0|Web Design from t...|   true|159.99|Learn web design ...|         1266.0|      4.75|       38.0|        12.0|        38.0|             569.0|2011-06-23T18:31:20Z|            null|   Design|         Web Design|   Web Design| English|/course/web-desig...| E Learning Lab|/user/edwin-ang-2/|\n+------+--------------------+-------+------+--------------------+---------------+----------+-----------+------------+------------+------------------+--------------------+----------------+---------+-------------------+-------------+--------+--------------------+---------------+------------------+\nonly showing top 5 rows\n\n+---------------+--------------+-----+--------+\n|instructor_name|instructor_url|topic|headline|\n+---------------+--------------+-----+--------+\n|              0|             0|    0|       0|\n+---------------+--------------+-----+--------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Convert 'id' column in the courses_df to integer\ncourses_df = courses_df.withColumn(\"id\", col(\"id\").cast(\"long\"))\n\n# Verify the schema to confirm the change\ncourses_df.printSchema()\n\n# Show a few rows to verify the change\ncourses_df.select(\"id\").show(5, truncate=False)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- id: long (nullable = true)\n |-- title: string (nullable = true)\n |-- is_paid: boolean (nullable = true)\n |-- price: double (nullable = true)\n |-- headline: string (nullable = true)\n |-- num_subscribers: double (nullable = true)\n |-- avg_rating: double (nullable = true)\n |-- num_reviews: double (nullable = true)\n |-- num_comments: double (nullable = true)\n |-- num_lectures: double (nullable = true)\n |-- content_length_min: double (nullable = true)\n |-- published_time: string (nullable = true)\n |-- last_update_date: string (nullable = true)\n |-- category: string (nullable = true)\n |-- subcategory: string (nullable = true)\n |-- topic: string (nullable = true)\n |-- language: string (nullable = true)\n |-- course_url: string (nullable = true)\n |-- instructor_name: string (nullable = true)\n |-- instructor_url: string (nullable = true)\n\n+----+\n|id  |\n+----+\n|4715|\n|1769|\n|5664|\n|7723|\n|8157|\n+----+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Count null values for each column in the comments table\ncomments_null_counts = comments_df.select(\n    [count(when(col(c).isNull(), c)).alias(c) for c in comments_df.columns]\n)\n\n# Show the null counts\ncomments_null_counts.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+---------+----+----+------------+-------+\n| id|course_id|rate|date|display_name|comment|\n+---+---------+----+----+------------+-------+\n|  0|        0|   0|   0|           0|      0|\n+---+---------+----+----+------------+-------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Count null-like values (empty strings, whitespaces) in specific columns\ncomments_null_like_counts = comments_df.select(\n    count(when((col(\"display_name\").isNull()) | (col(\"display_name\") == \"\") | (col(\"display_name\").rlike(\"^\\s*$\")), \"display_name\")).alias(\"display_name_nulls\"),\n    count(when((col(\"comment\").isNull()) | (col(\"comment\") == \"\") | (col(\"comment\").rlike(\"^\\s*$\")), \"comment\")).alias(\"comment_nulls\"),\n    count(when((col(\"course_id\").isNull()), \"course_id\")).alias(\"course_id_nulls\"),\n    count(when((col(\"rate\").isNull()), \"rate\")).alias(\"rate_nulls\")\n)\n\n# Show the null-like counts\ncomments_null_like_counts.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------------+-------------+---------------+----------+\n|display_name_nulls|comment_nulls|course_id_nulls|rate_nulls|\n+------------------+-------------+---------------+----------+\n|                10|           19|              0|         0|\n+------------------+-------------+---------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "comments_df.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- id: long (nullable = true)\n |-- course_id: long (nullable = true)\n |-- rate: double (nullable = true)\n |-- date: string (nullable = true)\n |-- display_name: string (nullable = true)\n |-- comment: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import count, when, col\n\n# Check for nulls or null-like values in the relevant columns\ncomments_df.select(\n    [count(when(col(c).isNull() | (col(c) == \"\") | (col(c).rlike(\"^\\s*$\")), c)).alias(c) for c in comments_df.columns]\n).show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+---------+----+----+------------+-------+\n| id|course_id|rate|date|display_name|comment|\n+---+---------+----+----+------------+-------+\n|  0|        0|   0|   0|          10|     19|\n+---+---------+----+----+------------+-------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Drop rows with nulls in critical columns\ncomments_df = comments_df.dropna(subset=[\"id\", \"course_id\", \"rate\"])\n\n# Fill null or empty values in non-critical columns\nfrom pyspark.sql.functions import when\n\ncomments_df = comments_df.fillna({\n    \"display_name\": \"Anonymous\",\n    \"comment\": \"No Comment\"\n})\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import trim, lower, col, to_date\n\n# Clean and standardize strings\ncomments_df = comments_df.withColumn(\"display_name\", trim(lower(col(\"display_name\")))) \\\n                         .withColumn(\"comment\", trim(col(\"comment\")))\n\n# Format date column\ncomments_df = comments_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "comments_df.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- id: long (nullable = true)\n |-- course_id: long (nullable = true)\n |-- rate: double (nullable = true)\n |-- date: date (nullable = true)\n |-- display_name: string (nullable = false)\n |-- comment: string (nullable = false)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Cast id to double\ncomments_df = comments_df.withColumn(\"id\", col(\"id\").cast(\"double\"))\n\n# Verify the schema to ensure the change\ncomments_df.printSchema()\n\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- id: double (nullable = true)\n |-- course_id: long (nullable = true)\n |-- rate: double (nullable = true)\n |-- date: date (nullable = true)\n |-- display_name: string (nullable = false)\n |-- comment: string (nullable = false)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Cast id, course_id, id_extracted, and course_id_extracted to double\ncomments_df = comments_df.withColumn(\"id\", col(\"id\").cast(\"double\")) \\\n                         .withColumn(\"course_id\", col(\"course_id\").cast(\"double\"))\n                          \n                        \n\n# Verify the schema to ensure the changes\ncomments_df.printSchema()\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- id: double (nullable = true)\n |-- course_id: double (nullable = true)\n |-- rate: double (nullable = true)\n |-- date: date (nullable = true)\n |-- display_name: string (nullable = false)\n |-- comment: string (nullable = false)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import to_date\n\n# Convert the date column to 'yyyy-MM-dd' format\ncomments_df = comments_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n\n# Show the transformed DataFrame\ncomments_df.show(5)\n\n# Verify the schema\ncomments_df.printSchema()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------+---------+----+----------+-------------+--------------------+\n|          id|course_id|rate|      date| display_name|             comment|\n+------------+---------+----+----------+-------------+--------------------+\n| 8.8962892E7|3173036.0| 1.0|2021-06-29|        rahul|I think a beginne...|\n| 1.2553547E8|4913148.0| 5.0|2022-10-07|        marlo|Aviva is such a n...|\n| 6.8767147E7|3178386.0| 3.5|2020-10-19|yamila andrea|Muy buena la intr...|\n|1.25029758E8|3175814.0| 5.0|2022-09-30|   jacqueline|This course is th...|\n| 7.6584052E7|3174896.0| 4.5|2021-01-30|      anthony|I found this cour...|\n+------------+---------+----+----------+-------------+--------------------+\nonly showing top 5 rows\n\nroot\n |-- id: double (nullable = true)\n |-- course_id: double (nullable = true)\n |-- rate: double (nullable = true)\n |-- date: date (nullable = true)\n |-- display_name: string (nullable = false)\n |-- comment: string (nullable = false)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import when\n\ncomments_df = comments_df.withColumn(\n    \"id\",\n    when(col(\"id\").cast(\"long\").isNotNull(), col(\"id\").cast(\"long\")).otherwise(-1)\n).withColumn(\n    \"course_id\",\n    when(col(\"course_id\").cast(\"long\").isNotNull(), col(\"course_id\").cast(\"long\")).otherwise(-1)\n)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Check the first few rows\ncomments_df.select(\"id\", \"course_id\").show(5, truncate=False)\n\n# Confirm the schema\ncomments_df.printSchema()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+---------+\n|id       |course_id|\n+---------+---------+\n|88962892 |3173036  |\n|125535470|4913148  |\n|68767147 |3178386  |\n|125029758|3175814  |\n|76584052 |3174896  |\n+---------+---------+\nonly showing top 5 rows\n\nroot\n |-- id: long (nullable = true)\n |-- course_id: long (nullable = true)\n |-- rate: double (nullable = true)\n |-- date: date (nullable = true)\n |-- display_name: string (nullable = false)\n |-- comment: string (nullable = false)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import count, when, col\n\n# Count null values for each column\ncomments_df.select(\n    [count(when(col(c).isNull(), c)).alias(c) for c in comments_df.columns]\n).show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+---------+----+----+------------+-------+\n| id|course_id|rate|date|display_name|comment|\n+---+---------+----+----+------------+-------+\n|  0|        0|   0|   0|           0|      0|\n+---+---------+----+----+------------+-------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Check for duplicate rows in courses table\nduplicates_in_courses = courses_df.groupBy(courses_df.columns).count().filter(col(\"count\") > 1)\n\n# Show duplicate rows\nduplicates_in_courses.show(truncate=False)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-----+-------+-----+--------+---------------+----------+-----------+------------+------------+------------------+--------------+----------------+--------+-----------+-----+--------+----------+---------------+--------------+-----+\n|id |title|is_paid|price|headline|num_subscribers|avg_rating|num_reviews|num_comments|num_lectures|content_length_min|published_time|last_update_date|category|subcategory|topic|language|course_url|instructor_name|instructor_url|count|\n+---+-----+-------+-----+--------+---------------+----------+-----------+------------+------------+------------------+--------------+----------------+--------+-----------+-----+--------+----------+---------------+--------------+-----+\n+---+-----+-------+-----+--------+---------------+----------+-----------+------------+------------+------------------+--------------+----------------+--------+-----------+-----+--------+----------+---------------+--------------+-----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Count total number of duplicate rows\ntotal_duplicates_courses = courses_df.groupBy(courses_df.columns).count().filter(col(\"count\") > 1).count()\n\nprint(f\"Total duplicate rows in courses table: {total_duplicates_courses}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "Total duplicate rows in courses table: 0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Check for duplicate rows in comments table\nfrom pyspark.sql.functions import count\n\nduplicates_in_comments = comments_df.groupBy(comments_df.columns).count().filter(col(\"count\") > 1)\n\n# Show duplicate rows\nduplicates_in_comments.show(truncate=False)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+---------+----+----+------------+-------+-----+\n|id |course_id|rate|date|display_name|comment|count|\n+---+---------+----+----+------------+-------+-----+\n+---+---------+----+----+------------+-------+-----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Count total number of duplicate rows\ntotal_duplicates_comments = comments_df.groupBy(comments_df.columns).count().filter(col(\"count\") > 1).count()\n\nprint(f\"Total duplicate rows in comments table: {total_duplicates_comments}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "Total duplicate rows in comments table: 0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "comments_df.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- id: long (nullable = true)\n |-- course_id: long (nullable = true)\n |-- rate: double (nullable = true)\n |-- date: date (nullable = true)\n |-- display_name: string (nullable = false)\n |-- comment: string (nullable = false)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "courses_df.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- id: long (nullable = true)\n |-- title: string (nullable = true)\n |-- is_paid: boolean (nullable = true)\n |-- price: double (nullable = true)\n |-- headline: string (nullable = true)\n |-- num_subscribers: double (nullable = true)\n |-- avg_rating: double (nullable = true)\n |-- num_reviews: double (nullable = true)\n |-- num_comments: double (nullable = true)\n |-- num_lectures: double (nullable = true)\n |-- content_length_min: double (nullable = true)\n |-- published_time: string (nullable = true)\n |-- last_update_date: string (nullable = true)\n |-- category: string (nullable = true)\n |-- subcategory: string (nullable = true)\n |-- topic: string (nullable = true)\n |-- language: string (nullable = true)\n |-- course_url: string (nullable = true)\n |-- instructor_name: string (nullable = true)\n |-- instructor_url: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import to_date\n\n# Convert published_time and last_update_date to 'yyyy-MM-dd' format\ncourses_df = courses_df.withColumn(\"published_time\", to_date(col(\"published_time\"), \"yyyy-MM-dd\")) \\\n                       .withColumn(\"last_update_date\", to_date(col(\"last_update_date\"), \"yyyy-MM-dd\"))\n\n# Verify the schema and data\ncourses_df.printSchema()\ncourses_df.select(\"published_time\", \"last_update_date\").show(5, truncate=False)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- id: long (nullable = true)\n |-- title: string (nullable = true)\n |-- is_paid: boolean (nullable = true)\n |-- price: double (nullable = true)\n |-- headline: string (nullable = true)\n |-- num_subscribers: double (nullable = true)\n |-- avg_rating: double (nullable = true)\n |-- num_reviews: double (nullable = true)\n |-- num_comments: double (nullable = true)\n |-- num_lectures: double (nullable = true)\n |-- content_length_min: double (nullable = true)\n |-- published_time: date (nullable = true)\n |-- last_update_date: date (nullable = true)\n |-- category: string (nullable = true)\n |-- subcategory: string (nullable = true)\n |-- topic: string (nullable = true)\n |-- language: string (nullable = true)\n |-- course_url: string (nullable = true)\n |-- instructor_name: string (nullable = true)\n |-- instructor_url: string (nullable = true)\n\n+--------------+----------------+\n|published_time|last_update_date|\n+--------------+----------------+\n|2010-08-05    |2020-11-06      |\n|2010-01-12    |null            |\n|2010-10-13    |2019-10-09      |\n|2011-06-20    |2016-01-13      |\n|2011-06-23    |null            |\n+--------------+----------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Filter out invalid rates\ncomments_df = comments_df.filter((col(\"rate\") >= 1) & (col(\"rate\") <= 5))\n\n# Filter out invalid prices\ncourses_df = courses_df.filter((col(\"price\") >= 0) & (col(\"price\") <= 1000))\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Count rows in comments table\ncomments_row_count = comments_df.count()\nprint(f\"Total rows in comments table: {comments_row_count}\")\n# Count rows in courses table\ncourses_row_count = courses_df.count()\nprint(f\"Total rows in courses table: {courses_row_count}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "Total rows in comments table: 99855\nTotal rows in courses table: 186788\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col\n\n# Rename id to course_id in courses_df\ncourses_df = courses_df.withColumnRenamed(\"id\", \"course_id\")\n\n# Ensure course_id data types match\ncomments_df = comments_df.withColumn(\"course_id\", col(\"course_id\").cast(\"long\"))\ncourses_df = courses_df.withColumn(\"course_id\", col(\"course_id\").cast(\"long\"))\n\n# Perform the join\nenriched_data = comments_df.join(courses_df, on=\"course_id\", how=\"inner\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "row_count = enriched_data.count()\nprint(f\"Total number of rows in enriched_data: {row_count}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "Total number of rows in enriched_data: 92870\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "courses_df.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- course_id: long (nullable = true)\n |-- title: string (nullable = true)\n |-- is_paid: boolean (nullable = true)\n |-- price: double (nullable = true)\n |-- headline: string (nullable = true)\n |-- num_subscribers: double (nullable = true)\n |-- avg_rating: double (nullable = true)\n |-- num_reviews: double (nullable = true)\n |-- num_comments: double (nullable = true)\n |-- num_lectures: double (nullable = true)\n |-- content_length_min: double (nullable = true)\n |-- published_time: date (nullable = true)\n |-- last_update_date: date (nullable = true)\n |-- category: string (nullable = true)\n |-- subcategory: string (nullable = true)\n |-- topic: string (nullable = true)\n |-- language: string (nullable = true)\n |-- course_url: string (nullable = true)\n |-- instructor_name: string (nullable = true)\n |-- instructor_url: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "duplicates_instructors = courses_df.groupBy(\"instructor_name\").count().filter(\"count > 1\")\nduplicates_instructors.show()\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+-----+\n|     instructor_name|count|\n+--------------------+-----+\n|         Angela Poch|    8|\n|Bplans School of ...|    5|\n|           Joe Saenz|    3|\n|      Rexcel Cariaga|    3|\n|      Daniel McCarty|    5|\n|      Kenney Mencher|    2|\n|           Mary Buck|    2|\n|        Lissa Coffey|    7|\n|Jef Gazley, M.S.,...|    8|\n|Ricardo Párraga Z...|    9|\n|        Peter Janzen|    3|\n|       Amani Channel|    2|\n|  Christelle Donaghy|    3|\n| Aspiratech Training|    7|\n|       Steve Reifman|    2|\n|  Intellezy Trainers|  245|\n|JMG Virtual Consu...|    8|\n|Lee Jones BSc. MS...|    3|\n|         Caleb Curry|    8|\n|    Cristian Carrera|    3|\n+--------------------+-----+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create Instructor Dimension using instructor_name and instructor_url\ninstructor_dim = courses_df.select(\n    col(\"instructor_name\"),\n    col(\"instructor_url\")\n).dropDuplicates()\n\n# Generate sequential IDs\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\n\nwindow_spec_instructor = Window.orderBy(\"instructor_name\", \"instructor_url\")\ninstructor_dim = instructor_dim.withColumn(\n    \"instructor_dim_id\", row_number().over(window_spec_instructor)\n)\n\ninstructor_dim.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------------+--------------------+-----------------+\n|          instructor_name|      instructor_url|instructor_dim_id|\n+-------------------------+--------------------+-----------------+\n|          \" NetworkHelp \"|/user/parmod-dhim...|                1|\n|     \"Mr. Casual\" Char...|/user/charlie-dec...|                2|\n|     \"Real Teacher\" ∼F...|      /user/mak-fav/|                3|\n|       #Bora Certificar ?|/user/isabella-so...|                4|\n|     #Digitalizzando A...|      /user/syrus-4/|                5|\n|     'Regen Ray' Milidoni|  /user/raymilidoni/|                6|\n|     (Bo)Siripa Aruenpong|/user/siripa-arue...|                7|\n|     (International Di...|/user/nicktoussaint/|                8|\n|        (Mila) Mengyun Yi|    /user/mengyun-8/|                9|\n|(あくしょん) Murakami ...|/user/murakami-yo...|               10|\n|  (株) ケン・ミュージック|/user/kenmiyuzits...|               11|\n|  (株)笑い総研 大久保信克|/user/da-jiu-bao-...|               12|\n|          + DoctorApple +|  /user/doctorapple/|               13|\n|                      - -| /user/martin-rolon/|               14|\n|           - ClouDevOps -|/user/abdelilah-h...|               15|\n|          - LaPrimera.net|/user/felixromanh...|               16|\n|            - SchoolSteps|  /user/schoolsteps/|               17|\n|           - Skillcoach -|   /user/skillcoach/|               18|\n|          - Superpadres -| /user/grupoplaneta/|               19|\n|           - Tadeo Correa| /user/tadeo-correa/|               20|\n+-------------------------+--------------------+-----------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import lit\n\n# Fill null instructor_url with a placeholder\ninstructor_dim = instructor_dim.fillna({\"instructor_url\": \"Unknown\"})\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create Course Dimension\ncourse_dim = courses_df.select(\n    col(\"course_id\"),\n    col(\"title\"),\n    col(\"price\"),\n    col(\"is_paid\"),\n    col(\"course_url\")\n).dropDuplicates()\n\n# Generate sequential IDs\nwindow_spec_course = Window.orderBy(\"course_id\")\ncourse_dim = course_dim.withColumn(\n    \"course_dim_id\", row_number().over(window_spec_course)\n)\n\ncourse_dim.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+--------------------+------+-------+--------------------+-------------+\n|course_id|               title| price|is_paid|          course_url|course_dim_id|\n+---------+--------------------+------+-------+--------------------+-------------+\n|     2762|Simple Strategy f...| 39.99|   true|/course/swing-tra...|            1|\n|     4715|Online Vegan Vege...| 24.99|   true|/course/vegan-veg...|            2|\n|     5664|How To Become a V...| 19.99|   true|/course/see-my-pe...|            3|\n|     7723|How to Train a Puppy|199.99|   true|/course/complete-...|            4|\n|     8069|    Curso SEO Online| 99.99|   true|/course/curso-de-...|            5|\n|     8075|How to Create an ...|149.99|   true|/course/how-to-cr...|            6|\n|     8082|Ruby Programming ...| 74.99|   true|/course/learn-rub...|            7|\n|     8139|14-Day Yoga Detox...| 29.99|   true|/course/yoga-for-...|            8|\n|     8157|Web Design from t...|159.99|   true|/course/web-desig...|            9|\n|     8318|Navigating the MB...| 49.99|   true|/course/business-...|           10|\n|     8319|Git Basics: In Th...| 19.99|   true|/course/git-in-th...|           11|\n|     8324|Javascript for Be...| 19.99|   true|/course/beginning...|           12|\n|     8410|Intro to Adobe In...| 24.99|   true|/course/intro-to-...|           13|\n|     8416|Beginners - How T...| 49.99|   true|/course/beginners...|           14|\n|     8420|CCNP ROUTE 300-10...| 19.99|   true|/course/ccnp-rout...|           15|\n|     8422|Kundalini Yoga to...| 49.99|   true|/course/kundalini...|           16|\n|     8467|    The Lean Startup| 39.99|   true|/course/the-lean-...|           17|\n|     8471|Healthy Cooking F...| 29.99|   true|/course/healthy-c...|           18|\n|     8621|Apps for Libraria...| 89.99|   true|/course/apps4libr...|           19|\n|     8629|Chemistry Funda (...| 34.99|   true|/course/chemistry...|           20|\n+---------+--------------------+------+-------+--------------------+-------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create Category Dimension\ncategory_dim = courses_df.select(\n    col(\"category\"),\n    col(\"subcategory\"),\n    col(\"topic\")\n).dropDuplicates()\n\n# Generate sequential IDs\nwindow_spec_category = Window.orderBy(\"category\", \"subcategory\", \"topic\")\ncategory_dim = category_dim.withColumn(\n    \"category_dim_id\", row_number().over(window_spec_category)\n)\n\ncategory_dim.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+--------------------+--------------------+---------------+\n|category|         subcategory|               topic|category_dim_id|\n+--------+--------------------+--------------------+---------------+\n|Business|Business Analytic...|         A/B Testing|              1|\n|Business|Business Analytic...|AWS Certified Mac...|              2|\n|Business|Business Analytic...|          Accounting|              3|\n|Business|Business Analytic...|               Agile|              4|\n|Business|Business Analytic...|          Algorithms|              5|\n|Business|Business Analytic...|             Alteryx|              6|\n|Business|Business Analytic...|          Amazon AWS|              7|\n|Business|Business Analytic...|   Amazon QuickSight|              8|\n|Business|Business Analytic...|           AngularJS|              9|\n|Business|Business Analytic...|            AnyLogic|             10|\n|Business|Business Analytic...|        Apache Kafka|             11|\n|Business|Business Analytic...|        Apache Storm|             12|\n|Business|Business Analytic...|Artificial Intell...|             13|\n|Business|Business Analytic...|AutoML Automated ...|             14|\n|Business|Business Analytic...|          Automation|             15|\n|Business|Business Analytic...|                BERT|             16|\n|Business|Business Analytic...|BPM Business Proc...|             17|\n|Business|Business Analytic...|BPMN Business Pro...|             18|\n|Business|Business Analytic...| Bayesian Statistics|             19|\n|Business|Business Analytic...|Behavioral Economics|             20|\n+--------+--------------------+--------------------+---------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create Language Dimension (Finalized - single instance)\nlanguage_dim = courses_df.select(\n    col(\"language\")\n).dropDuplicates()\n\n# Generate sequential IDs for language dimension\nwindow_spec_language = Window.orderBy(\"language\")\nlanguage_dim = language_dim.withColumn(\n    \"language_dim_id\", row_number().over(window_spec_language))\nlanguage_dim.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+---------------+\n| language|language_dim_id|\n+---------+---------------+\n|Afrikaans|              1|\n| Albanian|              2|\n|   Arabic|              3|\n| Armenian|              4|\n|   Aymara|              5|\n|    Azeri|              6|\n|   Basque|              7|\n|  Bengali|              8|\n|Bulgarian|              9|\n|  Burmese|             10|\n|  Catalan|             11|\n| Croatian|             12|\n|    Czech|             13|\n|   Danish|             14|\n|    Dutch|             15|\n|  English|             16|\n| Estonian|             17|\n|  Faroese|             18|\n| Filipino|             19|\n|  Finnish|             20|\n+---------+---------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Collect all unique dates from courses and comments tables\nfrom pyspark.sql.functions import to_date, col, year, month, dayofmonth, row_number\nunique_dates = (\n    courses_df.select(to_date(col(\"published_time\")).alias(\"date\"))\n    .union(courses_df.select(to_date(col(\"last_update_date\")).alias(\"date\")))\n    .union(comments_df.select(to_date(col(\"date\")).alias(\"date\")))\n    .dropDuplicates()\n)\n\n# Add year, month, and day columns\ndate_dim = unique_dates.withColumn(\"year\", year(col(\"date\"))) \\\n                       .withColumn(\"month\", month(col(\"date\"))) \\\n                       .withColumn(\"day\", dayofmonth(col(\"date\")))\n\n# Generate sequential IDs\nwindow_spec_date = Window.orderBy(\"date\")\ndate_dim = date_dim.withColumn(\n    \"date_dim_id\", row_number().over(window_spec_date)\n)\n\ndate_dim.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+----+-----+----+-----------+\n|      date|year|month| day|date_dim_id|\n+----------+----+-----+----+-----------+\n|      null|null| null|null|          1|\n|2010-04-14|2010|    4|  14|          2|\n|2010-08-05|2010|    8|   5|          3|\n|2010-10-13|2010|   10|  13|          4|\n|2011-06-20|2011|    6|  20|          5|\n|2011-06-23|2011|    6|  23|          6|\n|2011-07-06|2011|    7|   6|          7|\n|2011-07-08|2011|    7|   8|          8|\n|2011-07-09|2011|    7|   9|          9|\n|2011-07-11|2011|    7|  11|         10|\n|2011-07-12|2011|    7|  12|         11|\n|2011-07-15|2011|    7|  15|         12|\n|2011-07-18|2011|    7|  18|         13|\n|2011-07-23|2011|    7|  23|         14|\n|2011-07-28|2011|    7|  28|         15|\n|2011-07-29|2011|    7|  29|         16|\n|2011-08-03|2011|    8|   3|         17|\n|2011-08-08|2011|    8|   8|         18|\n|2011-08-13|2011|    8|  13|         19|\n|2011-08-22|2011|    8|  22|         20|\n+----------+----+-----+----+-----------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col\n\n# Join the dimensions with the courses_df to create the CoursePerformance Fact Table\ncourse_performance_fact = courses_df.alias(\"c\") \\\n    .join(course_dim.alias(\"cd\"), col(\"c.course_id\") == col(\"cd.course_id\"), \"inner\") \\\n    .join(instructor_dim.alias(\"id\"), (col(\"c.instructor_name\") == col(\"id.instructor_name\")) & \n         (col(\"c.instructor_url\") == col(\"id.instructor_url\")), \"inner\") \\\n    .join(category_dim.alias(\"catd\"), (col(\"c.category\") == col(\"catd.category\")) & \n         (col(\"c.subcategory\") == col(\"catd.subcategory\")) & \n         (col(\"c.topic\") == col(\"catd.topic\")), \"inner\") \\\n    .join(language_dim.alias(\"ld\"), col(\"c.language\") == col(\"ld.language\"), \"inner\") \\\n    .join(date_dim.alias(\"dd\"), to_date(col(\"c.published_time\")) == col(\"dd.date\"), \"inner\") \\\n    .select(\n        col(\"cd.course_dim_id\"),\n        col(\"ld.language_dim_id\"),\n        col(\"catd.category_dim_id\"),\n        col(\"id.instructor_dim_id\"),\n        col(\"dd.date_dim_id\").alias(\"published_date_dim_id\"),\n        col(\"c.num_subscribers\"),\n        col(\"c.avg_rating\"),\n        col(\"c.num_reviews\"),\n        col(\"c.num_comments\"),\n        col(\"c.num_lectures\"),\n        col(\"c.content_length_min\")\n    )\n\ncourse_performance_fact = course_performance_fact.withColumnRenamed(\"row_number()\", \"performance_fact_id\")\n\n# Display the CoursePerformance Fact Table\ncourse_performance_fact.show()\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------+---------------+---------------+-----------------+---------------------+---------------+----------+-----------+------------+------------+------------------+\n|course_dim_id|language_dim_id|category_dim_id|instructor_dim_id|published_date_dim_id|num_subscribers|avg_rating|num_reviews|num_comments|num_lectures|content_length_min|\n+-------------+---------------+---------------+-----------------+---------------------+---------------+----------+-----------+------------+------------+------------------+\n|            6|             16|           2619|            40607|                    7|        10761.0|       3.9|      349.0|       101.0|        87.0|             526.0|\n|           15|             16|          11085|            11359|                   14|         4454.0|      4.35|      829.0|       147.0|       230.0|            1813.0|\n|           53|             16|           3723|             8924|                   40|          743.0|       4.3|       87.0|        22.0|        33.0|             140.0|\n|           68|             16|          15443|            34187|                   45|        32813.0|  4.107595|     2824.0|       987.0|        44.0|             170.0|\n|           82|             16|          15440|            38898|                   55|          582.0|       4.3|       42.0|        19.0|        19.0|             178.0|\n|           93|             16|           2284|             8717|                   62|         5025.0|  4.196429|      604.0|       126.0|        90.0|             987.0|\n|           96|             16|          21050|             8355|                  125|        15652.0|  4.681818|     1913.0|       640.0|        54.0|             334.0|\n|          110|             16|           8334|            53556|                   70|           78.0|       5.0|        6.0|         2.0|        18.0|              60.0|\n|          130|             16|          16014|            28492|                   81|         3873.0|       3.2|       32.0|        21.0|        46.0|             280.0|\n|          138|             16|          17397|            26063|                   84|         1828.0|      4.55|      122.0|        40.0|        75.0|             430.0|\n|          179|             16|           4841|            57101|                  167|         2911.0|       3.9|       92.0|        24.0|        55.0|             368.0|\n|          205|             16|          15625|            53556|                  117|          131.0|       4.7|       24.0|        19.0|        65.0|             155.0|\n|          206|             16|           4962|             2963|                  113|         2348.0|      3.55|       19.0|         5.0|        55.0|             197.0|\n|          235|             16|          14474|            11060|                  136|         1475.0|      4.85|      140.0|        46.0|        52.0|             394.0|\n|          256|             16|           4929|            13293|                  283|         1170.0|      4.75|      100.0|        36.0|        47.0|             211.0|\n|          273|             16|          10016|            57640|                  149|          711.0|  4.357143|        7.0|         4.0|        31.0|             276.0|\n|          283|             16|           4461|            26063|                  155|        14014.0|       4.3|      370.0|       123.0|       119.0|             820.0|\n|          303|             16|           3743|             5221|                  215|         1837.0|      3.65|      463.0|        87.0|         6.0|              48.0|\n|          325|             61|          19069|            20715|                  181|          105.0|       4.0|       13.0|         9.0|        54.0|             127.0|\n|          331|             16|          11630|            31353|                  235|           69.0|       4.2|       20.0|        10.0|        42.0|             290.0|\n+-------------+---------------+---------------+-----------------+---------------------+---------------+----------+-----------+------------+------------+------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Unique courses in original dataset\nunique_courses = courses_df.select(\"course_id\").distinct().count()\nprint(f\"Total Unique Courses in Original Dataset: {unique_courses}\")\n\n# Total records in CoursePerformance Fact Table\nperformance_fact_count = course_performance_fact.count()\nprint(f\"Total Records in CoursePerformance Fact Table: {performance_fact_count}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "Total Unique Courses in Original Dataset: 186788\nTotal Records in CoursePerformance Fact Table: 186788\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "courses_df.select(\"course_id\").distinct().show(10, truncate=False)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+\n|course_id|\n+---------+\n|8075     |\n|8420     |\n|10971    |\n|12248    |\n|14706    |\n|15305    |\n|15392    |\n|16082    |\n|17158    |\n|17261    |\n+---------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "filtered_data = courses_df.filter(courses_df[\"course_id\"] == 8075)\nfiltered_data.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 50,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+--------------------+-------+------+--------------------+---------------+----------+-----------+------------+------------+------------------+--------------+----------------+--------+-----------+----------+--------+--------------------+----------------+--------------------+\n|course_id|               title|is_paid| price|            headline|num_subscribers|avg_rating|num_reviews|num_comments|num_lectures|content_length_min|published_time|last_update_date|category|subcategory|     topic|language|          course_url| instructor_name|      instructor_url|\n+---------+--------------------+-------+------+--------------------+---------------+----------+-----------+------------+------------+------------------+--------------+----------------+--------+-----------+----------+--------+--------------------+----------------+--------------------+\n|     8075|How to Create an ...|   true|149.99|You don't need to...|        10761.0|       3.9|      349.0|       101.0|        87.0|             526.0|    2011-07-06|      2020-11-22|Business|      Media|Demo Video| English|/course/how-to-cr...|Miguel Hernandez|/user/miguelherna...|\n+---------+--------------------+-------+------+--------------------+---------------+----------+-----------+------------+------------+------------------+--------------+----------------+--------+-----------+----------+--------+--------------------+----------------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\n\n# Create User Dimension table\nuser_dim = comments_df.select(\n    col(\"id\").alias(\"user_id\"),\n    col(\"display_name\")\n).dropDuplicates()\n\n# Generate surrogate key for users\nwindow_spec_user = Window.orderBy(\"user_id\")\nuser_dim = user_dim.withColumn(\n    \"user_dim_id\", row_number().over(window_spec_user)\n)\n\n# Display the User Dimension table\nuser_dim.show()\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+------------+-----------+\n|user_id|display_name|user_dim_id|\n+-------+------------+-----------+\n|   2591|         ian|          1|\n|   3791|      louise|          2|\n|   3833|       josef|          3|\n|   3841|      pamela|          4|\n|   3970|        adam|          5|\n|   4012|    abhishek|          6|\n|   4409|         jae|          7|\n|   4504|       david|          8|\n|   4664|       sarah|          9|\n|   4842|      trisha|         10|\n|   5253|       zenko|         11|\n|   5954|      samuel|         12|\n|   6164|       emily|         13|\n|   6302|       linda|         14|\n|   6735|         syd|         15|\n|   6741|       steve|         16|\n|   6802|     sultana|         17|\n|   6831|        mike|         18|\n|   6848|        jeff|         19|\n|   6959|        sean|         20|\n+-------+------------+-----------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Count the total number of unique user_dim_ids\ntotal_user_dim_ids = user_dim.select(\"user_dim_id\").distinct().count()\n\nprint(f\"Total number of unique user_dim_ids: {total_user_dim_ids}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 53,
			"outputs": [
				{
					"name": "stdout",
					"text": "Total number of unique user_dim_ids: 99855\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col, to_date\n\n# Ensure feedback date is in the correct format\ncomments_df = comments_df.withColumn(\"feedback_date\", to_date(col(\"date\")))\n\n# Join comments_df with course_dim, user_dim, and date_dim\ncourse_feedback_fact = comments_df.alias(\"cf\") \\\n    .join(course_dim.alias(\"cd\"), col(\"cf.course_id\") == col(\"cd.course_id\"), \"inner\") \\\n    .join(user_dim.alias(\"ud\"), col(\"cf.id\") == col(\"ud.user_id\"), \"inner\") \\\n    .join(date_dim.alias(\"dd\"), col(\"cf.feedback_date\") == col(\"dd.date\"), \"inner\") \\\n    .select(\n        col(\"cd.course_dim_id\"),\n        col(\"ud.user_dim_id\"),\n        col(\"dd.date_dim_id\").alias(\"feedback_date_dim_id\"),\n        col(\"cf.rate\"),\n        col(\"cf.comment\")\n    )\n\n\n# Step 4: Show result\ncourse_feedback_fact.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 54,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------+-----------+--------------------+----+-------------------------------------+\n|course_dim_id|user_dim_id|feedback_date_dim_id|rate|                              comment|\n+-------------+-----------+--------------------+----+-------------------------------------+\n|       173948|      92586|                3805| 1.0|                 nothing informati...|\n|       139093|      51003|                3497| 5.0|                 I would love to t...|\n|        88364|      51516|                3504| 5.0|小太郎シリーズ３コース受講しました...|\n|        88487|      68820|                3658| 5.0|                        Great course!|\n|        90472|      23876|                3077| 1.0|                 There was very li...|\n|       174323|      75156|                3706| 5.0|                 I got hooked on t...|\n|        90043|      24656|                3092| 3.0|                 السلام عليكم\\nاول...|\n|       139628|      47672|                3451| 5.0|                 Iniciar es la par...|\n|        85753|      34637|                3259| 1.0|音が悪く、小さくて聞き取りづらいの...|\n|       139309|      65493|                3631| 5.0|                 This was perfect ...|\n|        87848|      17934|                2955| 5.0|                 Curso de utilidad...|\n|       173421|      73731|                3695| 5.0|                 Explained it very...|\n|        86919|      78635|                3729| 3.0|                 Bilgiler kendini ...|\n|        86760|      20698|                3012| 4.0|                 It was good and k...|\n|        86999|      36376|                3288| 5.0|                 O Messias manja m...|\n|       173392|      77440|                3721| 2.0|                 perguntas totalme...|\n|        93306|      20818|                3014| 5.0|                 Fue una excelente...|\n|        93484|      38609|                3321| 3.5|                 this is good cour...|\n|       175366|      90904|                3797| 5.0|                       Great teacher.|\n|       175359|      75658|                3709| 5.0|                 IT alanında kendi...|\n+-------------+-----------+--------------------+----+-------------------------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "fact_count = course_feedback_fact.count()\nprint(f\"Total rows in fact table: {fact_count}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 55,
			"outputs": [
				{
					"name": "stdout",
					"text": "Total rows in fact table: 92870\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Find unmatched course_ids in comments_df\nunmatched_courses = comments_df.select(\"course_id\").subtract(course_dim.select(\"course_id\"))\nprint(f\"Unmatched course_id count: {unmatched_courses.count()}\")\n\n# Find unmatched user_ids in comments_df\nunmatched_users = comments_df.select(\"id\").subtract(user_dim.select(\"user_id\"))\nprint(f\"Unmatched user_id count: {unmatched_users.count()}\")\n\n# Find unmatched feedback dates in comments_df\nunmatched_dates = comments_df.select(to_date(col(\"date\")).alias(\"feedback_date\")) \\\n    .subtract(date_dim.select(\"date\"))\nprint(f\"Unmatched feedback_date count: {unmatched_dates.count()}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 56,
			"outputs": [
				{
					"name": "stdout",
					"text": "Unmatched course_id count: 3500\nUnmatched user_id count: 0\nUnmatched feedback_date count: 0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "unmatched_course_ids = comments_df.select(\"course_id\").subtract(course_dim.select(\"course_id\"))\nprint(f\"Number of unmatched course_id: {unmatched_course_ids.count()}\")\nunmatched_course_ids.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 57,
			"outputs": [
				{
					"name": "stdout",
					"text": "Number of unmatched course_id: 3500\n+---------+\n|course_id|\n+---------+\n|  3894456|\n|  4106512|\n|  4813950|\n|  4457246|\n|  4175888|\n|  2957632|\n|  4160760|\n|  3885136|\n|  3992348|\n|  3808988|\n|  4341190|\n|  3688594|\n|  4122310|\n|  4500434|\n|  3955796|\n|  4206800|\n|  4389016|\n|  4503836|\n|  4170294|\n|  4540588|\n+---------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "original_course_id = course_dim.filter(col(\"course_dim_id\") == 173948).select(\"course_id\").collect()[0][0]\nprint(f\"Original Course ID: {original_course_id}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 58,
			"outputs": [
				{
					"name": "stdout",
					"text": "Original Course ID: 4693438\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "original_user_id = user_dim.filter(col(\"user_dim_id\") == 92586).select(\"user_id\").collect()[0][0]\nprint(f\"Original User ID: {original_user_id}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 59,
			"outputs": [
				{
					"name": "stdout",
					"text": "Original User ID: 124129784\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "original_date = date_dim.filter(col(\"date_dim_id\") == 3805).select(\"date\").collect()[0][0]\nprint(f\"Original Date: {original_date}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 60,
			"outputs": [
				{
					"name": "stdout",
					"text": "Original Date: 2022-09-20\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "comments_df.filter(\n    (col(\"course_id\") == original_course_id) &\n    (col(\"id\") == original_user_id) &\n    (to_date(col(\"date\")) == original_date)\n).show(truncate=False)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 61,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+---------+----+----------+------------+--------------------------+-------------+\n|id       |course_id|rate|date      |display_name|comment                   |feedback_date|\n+---------+---------+----+----------+------------+--------------------------+-------------+\n|124129784|4693438  |1.0 |2022-09-20|jiang       |nothing information there,|2022-09-20   |\n+---------+---------+----+----------+------------+--------------------------+-------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Course Dimension\ncourse_dim.write.mode(\"overwrite\").parquet(\"s3://udemy-data-project/processed/course_dim/\")\n\n# User Dimension\nuser_dim.write.mode(\"overwrite\").parquet(\"s3://udemy-data-project/processed/user_dim/\")\n\n# Category Dimension\ncategory_dim.write.mode(\"overwrite\").parquet(\"s3://udemy-data-project/processed/category_dim/\")\n\n# Language Dimension\nlanguage_dim.write.mode(\"overwrite\").parquet(\"s3://udemy-data-project/processed/language_dim/\")\n\n# Date Dimension\ndate_dim.write.mode(\"overwrite\").parquet(\"s3://udemy-data-project/processed/date_dim/\")\n\n# Instructor Dimension\ninstructor_dim.write.mode(\"overwrite\").parquet(\"s3://udemy-data-project/processed/instructor_dim/\")\n\n# CourseFeedback Fact Table\ncourse_feedback_fact.write.mode(\"overwrite\").parquet(\"s3://udemy-data-project/processed/course_feedback_fact/\")\n\n# CoursePerformance Fact Table (if applicable)\ncourse_performance_fact.write.mode(\"overwrite\").parquet(\"s3://udemy-data-project/processed/course_performance_fact/\")\n\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 62,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://bucket_name/folder_name\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"demo\", catalogTableName=\"populations\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(DyF)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}